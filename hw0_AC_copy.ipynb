{"cells":[{"cell_type":"markdown","metadata":{"id":"xJky9yK6NEMZ"},"source":["# 546 hw0 -- Optimization and Learning for Control\n","\n","This assignment will be graded on participation; so long as you make an effort on each problem and upload a legible pdf to Canvas, you will receive full credit."]},{"cell_type":"markdown","metadata":{"id":"RtVc47IDNEMo"},"source":["# welcome!\n","\n","***Purpose:*** this sequence of questions will help us understand your background and goals for the course so that we can tailor the material accordingly.\n","\n","a. Where are you at in your current program?  (e.g. 3rd year PhD student in ECE), what were you doing before you enrolled in your current program, and what do you hope to do after you graduate?\n","\n","b. Why did you enroll in this course, what do you hope to learn, and how do you plan to apply what you've learned?\n","\n","c. What is your preferred toolkit for scientific computing?  (e.g. Python, Julia, Matlab, Mathematica, ...)\n","\n","d. What office hours (Instructor and/or TA) are you generally able to attend?  (If none, what is your availability?)\n","\n","e. Log in to Canvas and edit your Profile.  At a minimum, add a headshot photo and specify your preferred name and pronouns.  Ideally, also add some details about your background and interests -- see my example at https://canvas.uw.edu/about/3510568.  (Note that this information is available to classmates and instructors in this and the other courses you're enrolled in at UW; it is not publicly available.)"]},{"cell_type":"markdown","metadata":{},"source":["**a.** 2nd year PhD student in ECE. I studied bioengineering before joining UW.\n","\n","**b.** I would like to learn more about optimization and learning in order to apply them to my research in human-in-the-loop. And I would like to read more papers in the field of adaptation and(or) game theory. \n","\n","**c.** Python, Mtalab\n","\n","**d.** both"]},{"cell_type":"markdown","metadata":{"id":"_diA7xnKNEMr"},"source":["# reading\n","\n","***Purpose:*** we will discuss this paper in class on Thu Jan 6; that discussion, together with your responses below, will be used to steer lecture topics in the coming weeks.\n","\n","a. ***By 1p Tue Jan 11 2020:*** read this survey paper:\n","https://paperpile.com/shared/BHnJOI\n","\n","b. What ideas or concepts did you learn?\n","\n","c. What ideas or concepts were confusing / unclear / require further reading?\n","\n","d. Which (if any) of the ideas or concepts from (a.) and (b.) do you want to learn more about?\n","\n","e. Specify 1--3 citations from the paper that you want to learn more about."]},{"cell_type":"markdown","metadata":{},"source":["**a.** done\n","\n","**b.** I wasn't familiar with the differences between the model-free and the model-based learning, the paper gave a good high-level explanation on them, which I think makses sense. I have seen the equation of Q-learning before; however, I did not know the derivation of it and its connection to the Bellman's equation, and how \"policy\" plays a role in it. Moreover, I appreciate that the paper gives a straightforward LQR example to demonstrate the applications of different method. \n","\n","**c. & d.** In Q-function, I still don't understand the role of the \"discount factor\" in the reward problem. Why isn't the average reward a good estimation?  Also, I would appreciate detailed explanation on the probabilistic policy in equation 14.\n","\n","**e.** In the paper, it discussed that human-in-the-loop problems can be seen as inverse optimal control or inverse RL. I would like to read those two papers (Kalman, 1964; Ng AY, Russell, 2000) to understand how they model the rewards while human is hard to \"model\".  "]}],"metadata":{"anaconda-cloud":{},"colab":{"collapsed_sections":[],"name":"hw0.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"}},"nbformat":4,"nbformat_minor":0}
